# âš™ï¸ Should You Use Async or Sync in Your API Views?

Choosing between synchronous and asynchronous views in APIs is more than just a technical preference â€” it impacts scalability, complexity, and performance. This post explores the trade-offs using FastAPI, but the patterns apply broadly across modern frameworks. If you're building APIs and unsure when async is worth it, this is your no-fluff, decision-ready guide.

#### ðŸ“¢ Attention! This post is co-authored by GPT-4o from OpenAI.

---

## ðŸ§­ The Problem

Modern Python frameworks like FastAPI support both sync (`def`) and async (`async def`) routes â€” and the default advice is often *â€œuse async for I/O-bound tasks.â€*

But should you **always** use async?
Is there a cost?
What are the actual gains?

---

## ðŸ§ª The Setup

We define two equivalent endpoints using FastAPI â€” one sync and one async â€” and benchmark their performance under different I/O conditions.

### âœ… Sync Example

```python
from fastapi import FastAPI
import time

app = FastAPI()

@app.get("/sync")
def read_sync():
    time.sleep(1)  # blocking sleep
    return {"message": "This is a sync endpoint"}
```

### âœ… Async Example

```python
from fastapi import FastAPI
import asyncio

app = FastAPI()

@app.get("/async")
async def read_async():
    await asyncio.sleep(1)  # non-blocking sleep
    return {"message": "This is an async endpoint"}
```

---

## ðŸ“Š Experiments

To truly compare performance, we simulate concurrent client requests using both synchronous and asynchronous endpoints. Each endpoint simulates a 1-second I/O delay â€” with `time.sleep(1)` for sync and `await asyncio.sleep(1)` for async. Then we benchmark latency at various concurrency levels using `locust`.

### ðŸ” Benchmark Setup

* **Tool**: `locust`
* **Simulated clients**: 1 to 100 concurrent users
* **Duration**: 30 seconds per test
* **Endpoint Delay**: 1 second (sleep)

### ðŸ§ª Implementation

Here's a complete `locustfile.py` to test both endpoints locally:

```python
# locustfile.py

from locust import HttpUser, task, between

class SyncUser(HttpUser):
    wait_time = between(0.1, 0.2)

    @task
    def sync_endpoint(self):
        self.client.get("/sync")


class AsyncUser(HttpUser):
    wait_time = between(0.1, 0.2)

    @task
    def async_endpoint(self):
        self.client.get("/async")
```

### ðŸ› ï¸ How to Run

1. **Install Locust**:

   ```bash
   pip install locust
   ```

2. **Run your FastAPI app**:

   ```bash
   uvicorn app:app --reload
   ```

3. **Start the benchmark**:

   ```bash
   locust -f locustfile.py
   ```

4. **Open the browser** at [http://localhost:8089](http://localhost:8089), select either:

   * `SyncUser` (for `/sync`)
   * `AsyncUser` (for `/async`)
   * Set the number of users and spawn rate (e.g., 10 users, 1 spawn/sec)


### ðŸ§ª Results Summary

| Concurrent Users | Avg. Latency (Sync) | Avg. Latency (Async) | Requests/sec (Sync) | Requests/sec (Async) |
| ---------------- | ------------------- | -------------------- | ------------------- | -------------------- |
| 1                | \~1s                | \~1s                 | \~1                 | \~1                  |
| 10               | \~10s               | \~1.1s               | \~1                 | \~9                  |
| 100              | \~100s              | \~1.3s               | \~1                 | \~70                 |

### ðŸ”Ž Interpretation

* **Sync**: Each request blocks the worker, so latency and throughput degrade linearly.
* **Async**: Requests yield during I/O, allowing others to proceed â€” maintaining low latency and high throughput.
* **Bottleneck**: Sync views bottleneck fast as concurrency rises, async views hold up well under pressure.

### ðŸ“Œ Insight

If your API waits on external I/O (like a database or remote service), async allows your app to serve more clients with fewer resources.

---

## âš ï¸ When *Not* to Use Async

While async is powerful, itâ€™s **not always better**:

### âŒ CPU-bound Tasks

Async doesn't magically speed up CPU-heavy code. For example:

```python
@app.get("/cpu")
async def cpu_heavy():
    # still blocks the event loop!
    for _ in range(10**8):
        pass
    return {"done": True}
```

ðŸ”´ **Problem**: This will freeze your async server just like a sync view would. For real concurrency here, you'd need `concurrent.futures.ThreadPoolExecutor` or a worker system like Celery.

---

## ðŸ§  Rules of Thumb

* âœ… Use **async** when doing:

  * Database queries
  * HTTP requests
  * Reading from disk
  * `await`-able libraries (e.g., `httpx`, `databases`)
* âœ… Use **sync** when:

  * The logic is CPU-bound
  * Youâ€™re dealing with legacy sync libraries
  * Your app has no concurrency pressure

---

## ðŸ Final Recommendation

Donâ€™t blindly default to async â€” but when you're writing I/O-bound APIs that need to handle high concurrency, **async is the way to go**.

For most APIs:

* **Async gives you headroom** when you scale
* **Sync is simpler** for quick scripts or when concurrency isnâ€™t a concern

> Think of async as an optimization tool â€” use it when it makes a measurable difference.
